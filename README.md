In this research, the performance and robustness of Quantum Deep Q-learning Networks (DQN) were examined in two classical environments, Cart Pole and Lunar Lander, 
by using three distinct quantum Ansatz layers: RealAmplitudes, EfficientSU2, and TwoLocal. 
The quantum DQNs were compared with classical DQN algorithms in terms of convergence speed, loss minimization, and Q-value behavior. 
It was observed that the RealAmplitudes Ansatz outperformed the other quantum circuits, demonstrating faster convergence and superior performance in minimizing the loss function. 
To assess robustness, dynamic environments were introduced, where the pole length was increased after the 50th episode in the Cart Pole environment and a wind function was added to 
the Lunar Lander environment. 
All three quantum Ansatz layers were found to maintain robust performance under these dynamic conditions, with consistent reward values, loss minimization, and stable Q-value distributions.
